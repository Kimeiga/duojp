#!/usr/bin/env python3
"""
Compare different Chinese tokenizers on a comprehensive set of test sentences.
Tests: jieba, stanza, thulac, ltp, spacy-pkuseg
"""
import json
import time
from typing import List, Dict, Callable, Tuple

# Test sentences with diverse grammatical structures
TEST_SENTENCES = [
    # Basic sentences
    ("æˆ‘å–œæ¬¢åƒè‹¹æžœã€‚", "Basic SVO"),
    ("ä»–æ˜¯æˆ‘çš„æœ‹å‹ã€‚", "Copula"),
    ("å¥¹å¾ˆæ¼‚äº®ã€‚", "Adjective predicate"),
    
    # 4-character idioms (æˆè¯­) - KEY TEST CASES
    ("æ¢¦æƒ³æˆçœŸã€‚", "Idiom: dreams come true"),
    ("æˆ‘å¸Œæœ›æˆ‘çš„æ¢¦æƒ³æˆçœŸã€‚", "Idiom in context"),
    ("ä»–ä¸€å¿ƒä¸€æ„åœ°å­¦ä¹ ã€‚", "Idiom: wholeheartedly"),
    ("è¿™ä»¶äº‹æƒ…ä¸€ç›®äº†ç„¶ã€‚", "Idiom: clear at a glance"),
    ("æˆ‘ä»¬åº”è¯¥å®žäº‹æ±‚æ˜¯ã€‚", "Idiom: seek truth from facts"),
    ("ä»–çš„æˆç»©çªé£žçŒ›è¿›ã€‚", "Idiom: by leaps and bounds"),
    ("å¥¹å¿ƒæ»¡æ„è¶³åœ°ç¬‘äº†ã€‚", "Idiom: satisfied"),
    
    # Compound words
    ("æˆ‘åœ¨å›¾ä¹¦é¦†çœ‹ä¹¦ã€‚", "Compound: library"),
    ("ä»–æ˜¯ä¸€ä¸ªç”µè„‘ç¨‹åºå‘˜ã€‚", "Compound: computer programmer"),
    ("å¥¹ä¹°äº†ä¸€å°æ´—è¡£æœºã€‚", "Compound: washing machine"),
    ("æˆ‘ä»¬è¦ä¿æŠ¤çŽ¯å¢ƒã€‚", "Compound: environment"),
    ("äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚", "Compound: artificial intelligence"),
    
    # Names
    ("åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚", "Place: Beijing, China"),
    ("æŽæ˜ŽåŽ»ä¸Šæµ·å‡ºå·®äº†ã€‚", "Person + place name"),
    ("å¼ ä¼Ÿå’ŒçŽ‹èŠ³ç»“å©šäº†ã€‚", "Two person names"),
    
    # Numbers and time
    ("æˆ‘æœ‰ä¸‰ä¸ªè‹¹æžœã€‚", "Numbers"),
    ("ä¼šè®®åœ¨ä¸‹åˆä¸‰ç‚¹å¼€å§‹ã€‚", "Time expression"),
    
    # Questions
    ("ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ", "Question: what"),
    ("ä»–ä¸ºä»€ä¹ˆä¸æ¥ï¼Ÿ", "Question: why"),
    ("è¿™æœ¬ä¹¦å¤šå°‘é’±ï¼Ÿ", "Question: how much"),
    
    # Aspect particles
    ("æˆ‘åƒè¿‡ä¸­å›½èœã€‚", "Experiential: è¿‡"),
    ("ä»–æ­£åœ¨çœ‹ç€ç”µè§†ã€‚", "Progressive: ç€"),
    ("å¥¹å·²ç»èµ°äº†ã€‚", "Completed: äº†"),
    
    # Passive/causative
    ("è¿™æœ¬ä¹¦è¢«ä»–å€Ÿèµ°äº†ã€‚", "Passive: è¢«"),
    ("è€å¸ˆè®©å­¦ç”Ÿå†™ä½œä¸šã€‚", "Causative: è®©"),
    
    # Complex sentences  
    ("è™½ç„¶ä»–å¾ˆç´¯ï¼Œä½†æ˜¯ä»–è¿˜æ˜¯ç»§ç»­å·¥ä½œã€‚", "Although...but"),
    ("å¦‚æžœæ˜Žå¤©ä¸‹é›¨ï¼Œæˆ‘ä»¬å°±ä¸åŽ»äº†ã€‚", "If...then"),
    ("å› ä¸ºäº¤é€šå µå¡žï¼Œæ‰€ä»¥æˆ‘è¿Ÿåˆ°äº†ã€‚", "Because...so"),
    
    # Resultative complements
    ("æˆ‘å¬æ‡‚äº†ä»–è¯´çš„è¯ã€‚", "Resultative: understood"),
    ("ä»–æŠŠä½œä¸šåšå®Œäº†ã€‚", "Resultative with æŠŠ"),
    
    # Colloquial
    ("æ²¡é—®é¢˜ï¼", "Colloquial: no problem"),
    ("å¤ªæ£’äº†ï¼", "Colloquial: awesome"),
    ("æˆ‘è§‰å¾—æ²¡ä»€ä¹ˆå¤§ä¸äº†çš„ã€‚", "Colloquial: not a big deal"),
]

# Expected "ideal" tokenizations for key sentences (for scoring)
EXPECTED_TOKENIZATIONS = {
    "æ¢¦æƒ³æˆçœŸã€‚": ["æ¢¦æƒ³", "æˆçœŸ"],  # Should split idiom
    "æˆ‘å¸Œæœ›æˆ‘çš„æ¢¦æƒ³æˆçœŸã€‚": ["æˆ‘", "å¸Œæœ›", "æˆ‘", "çš„", "æ¢¦æƒ³", "æˆçœŸ"],
    "å›¾ä¹¦é¦†": ["å›¾ä¹¦é¦†"],  # Should NOT split
    "ç”µè„‘ç¨‹åºå‘˜": ["ç”µè„‘", "ç¨‹åºå‘˜"],  # Can split
    "æ´—è¡£æœº": ["æ´—è¡£æœº"],  # Should NOT split
    "äººå·¥æ™ºèƒ½": ["äººå·¥æ™ºèƒ½"],  # Should NOT split (or äººå·¥ + æ™ºèƒ½ ok)
    "åŒ—äº¬": ["åŒ—äº¬"],  # Should NOT split
    "ä¸Šæµ·": ["ä¸Šæµ·"],  # Should NOT split
}


def load_tokenizers():
    """Load all available tokenizers."""
    tokenizers = {}
    
    # 1. jieba
    try:
        import jieba
        jieba.setLogLevel(20)  # Suppress loading messages
        def jieba_tokenize(text):
            return list(jieba.cut(text))
        tokenizers['jieba'] = jieba_tokenize
        print("âœ“ jieba loaded")
    except ImportError:
        print("âœ— jieba not available")
    
    # 2. stanza
    try:
        import stanza
        nlp = stanza.Pipeline('zh', processors='tokenize', verbose=False)
        def stanza_tokenize(text):
            doc = nlp(text)
            return [token.text for sent in doc.sentences for token in sent.tokens]
        tokenizers['stanza'] = stanza_tokenize
        print("âœ“ stanza loaded")
    except Exception as e:
        print(f"âœ— stanza not available: {e}")
    
    # 3. thulac
    try:
        import thulac
        thu = thulac.thulac(seg_only=True)
        def thulac_tokenize(text):
            result = thu.cut(text, text=True)
            return result.split()
        tokenizers['thulac'] = thulac_tokenize
        print("âœ“ thulac loaded")
    except Exception as e:
        print(f"âœ— thulac not available: {e}")
    
    # 4. ltp
    try:
        from ltp import LTP
        ltp = LTP('LTP/small')
        def ltp_tokenize(text):
            output = ltp.pipeline([text], tasks=['cws'])
            return output.cws[0]
        tokenizers['ltp'] = ltp_tokenize
        print("âœ“ ltp loaded")
    except Exception as e:
        print(f"âœ— ltp not available: {e}")
    
    # 5. spacy-pkuseg
    try:
        import spacy_pkuseg as pkuseg
        seg = pkuseg.pkuseg()
        def pkuseg_tokenize(text):
            return seg.cut(text)
        tokenizers['pkuseg'] = pkuseg_tokenize
        print("âœ“ pkuseg loaded")
    except Exception as e:
        print(f"âœ— pkuseg not available: {e}")
    
    return tokenizers


def filter_punctuation(tokens: List[str]) -> List[str]:
    """Remove punctuation tokens for cleaner comparison."""
    punct = set("ã€‚ï¼Œã€ï¼›ï¼šï¼Ÿï¼â€¦â€”Â·ã€Œã€ã€Žã€ï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€ˆã€‰.?,!;:()")
    return [t for t in tokens if t.strip() and t not in punct]


def run_comparison(tokenizers: Dict[str, Callable]):
    """Run all tokenizers on all test sentences."""
    results = []
    
    for sentence, description in TEST_SENTENCES:
        result = {
            'sentence': sentence,
            'description': description,
            'tokenizations': {}
        }
        
        for name, tokenize_fn in tokenizers.items():
            try:
                tokens = tokenize_fn(sentence)
                tokens_clean = filter_punctuation(tokens)
                result['tokenizations'][name] = tokens_clean
            except Exception as e:
                result['tokenizations'][name] = [f"ERROR: {e}"]
        
        results.append(result)
    
    return results


def analyze_results(results: List[Dict]) -> Dict:
    """Analyze tokenization results and score each tokenizer."""
    tokenizer_names = list(results[0]['tokenizations'].keys())
    
    # Scoring categories
    scores = {name: {
        'idiom_split': 0,      # Properly splits idioms
        'compound_preserve': 0, # Preserves compound words
        'name_preserve': 0,     # Preserves names
        'consistency': 0,       # Consistent behavior
        'total_tokens': 0,
    } for name in tokenizer_names}
    
    # Analyze idiom handling (sentences 3-10)
    idiom_sentences = [r for r in results if 'Idiom' in r['description']]
    for r in idiom_sentences:
        for name, tokens in r['tokenizations'].items():
            # Check if idioms are split (good for learning)
            token_str = ''.join(tokens)
            # Count 2-char segments vs 4-char segments
            has_2char_split = any(len(t) == 2 for t in tokens if len(t) <= 4)
            if has_2char_split:
                scores[name]['idiom_split'] += 1
    
    # Analyze compound word handling
    compound_sentences = [r for r in results if 'Compound' in r['description']]
    for r in compound_sentences:
        for name, tokens in r['tokenizations'].items():
            # Check if key compounds are preserved
            if 'å›¾ä¹¦é¦†' in tokens:
                scores[name]['compound_preserve'] += 1
            if 'æ´—è¡£æœº' in tokens:
                scores[name]['compound_preserve'] += 1
    
    # Analyze name handling
    name_sentences = [r for r in results if 'name' in r['description'].lower() or 'Place' in r['description']]
    for r in name_sentences:
        for name, tokens in r['tokenizations'].items():
            if 'åŒ—äº¬' in tokens:
                scores[name]['name_preserve'] += 1
            if 'ä¸Šæµ·' in tokens:
                scores[name]['name_preserve'] += 1
            if 'ä¸­å›½' in tokens:
                scores[name]['name_preserve'] += 1
    
    # Count total tokens (lower might mean better grouping)
    for r in results:
        for name, tokens in r['tokenizations'].items():
            scores[name]['total_tokens'] += len(tokens)
    
    return scores


def print_results(results: List[Dict], scores: Dict):
    """Print formatted comparison results."""
    tokenizer_names = list(results[0]['tokenizations'].keys())
    
    print("\n" + "=" * 100)
    print("TOKENIZER COMPARISON RESULTS")
    print("=" * 100)
    
    # Print each sentence's tokenization
    for r in results:
        print(f"\nðŸ“ {r['sentence']}")
        print(f"   ({r['description']})")
        for name in tokenizer_names:
            tokens = r['tokenizations'][name]
            print(f"   {name:10}: {tokens}")
    
    # Print scores
    print("\n" + "=" * 100)
    print("SCORING SUMMARY")
    print("=" * 100)
    print(f"\n{'Tokenizer':<12} {'Idiom Split':<12} {'Compound':<12} {'Names':<12} {'Total Tokens':<12}")
    print("-" * 60)
    for name in tokenizer_names:
        s = scores[name]
        print(f"{name:<12} {s['idiom_split']:<12} {s['compound_preserve']:<12} {s['name_preserve']:<12} {s['total_tokens']:<12}")
    
    # Recommendations
    print("\n" + "=" * 100)
    print("ANALYSIS")
    print("=" * 100)
    
    # Find best for idiom splitting
    best_idiom = max(tokenizer_names, key=lambda n: scores[n]['idiom_split'])
    print(f"\nðŸ† Best for splitting idioms: {best_idiom} (score: {scores[best_idiom]['idiom_split']})")
    
    # Find best for compound preservation
    best_compound = max(tokenizer_names, key=lambda n: scores[n]['compound_preserve'])
    print(f"ðŸ† Best for preserving compounds: {best_compound} (score: {scores[best_compound]['compound_preserve']})")
    
    # Find best for name preservation
    best_names = max(tokenizer_names, key=lambda n: scores[n]['name_preserve'])
    print(f"ðŸ† Best for preserving names: {best_names} (score: {scores[best_names]['name_preserve']})")


def main():
    print("Loading tokenizers...")
    print("-" * 40)
    tokenizers = load_tokenizers()
    
    if not tokenizers:
        print("No tokenizers available!")
        return
    
    print(f"\nLoaded {len(tokenizers)} tokenizers: {list(tokenizers.keys())}")
    print(f"Testing {len(TEST_SENTENCES)} sentences...")
    
    results = run_comparison(tokenizers)
    scores = analyze_results(results)
    print_results(results, scores)
    
    # Save detailed results to JSON
    output_path = '/tmp/tokenizer_comparison.json'
    with open(output_path, 'w') as f:
        json.dump({'results': results, 'scores': scores}, f, ensure_ascii=False, indent=2)
    print(f"\nDetailed results saved to: {output_path}")


if __name__ == "__main__":
    main()

